# Adversarial Attack on MNIST (FGSM)

## ğŸ“Œ Project Overview

This project demonstrates a practical implementation of the **Fast Gradient Sign Method (FGSM)** adversarial attack on a Convolutional Neural Network (CNN) trained on the MNIST dataset.

The goal is to slightly perturb an input image in a direction that maximizes model loss, causing the model to misclassify the image â€” even though it still looks unchanged to a human.

---

## ğŸ¯ Objective

- Train a CNN model on MNIST
- Generate adversarial noise using FGSM
- Create adversarial examples
- Demonstrate successful misclassification

---

## ğŸ§  Key Concept

FGSM formula:

x_adv = x + Îµ * sign(âˆ‡x J(x, y))

Where:
- x = original image
- Îµ = perturbation strength
- âˆ‡x J(x, y) = gradient of loss w.r.t input
- sign() = direction of maximum increase

---

## ğŸ— Project Structure

adversarial-mnist/
â”‚
â”œâ”€â”€ train_model.py
â”œâ”€â”€ attack.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ mnist_victim_model.keras
â”‚
â””â”€â”€ screenshots/
â”œâ”€â”€ original.png
â”œâ”€â”€ adversarial.png
â””â”€â”€ perturbation.png


---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install Dependencies

pip install tensorflow numpy matplotlib


Or use:

pip install -r requirements.txt


---

### 2ï¸âƒ£ Train the Model

python train_model.py


This creates:

models/mnist_victim_model.keras


---

### 3ï¸âƒ£ Run the Attack

python attack.py


This generates:

- original.png
- adversarial.png
- perturbation.png

---

## ğŸ“Š Results

### Original Image
Model correctly predicts digit 9.

![Original](screenshots/original.png)

---

### Adversarial Image
The same image with minimal perturbation.
Model misclassifies digit 9 as digit 4.

![Adversarial](screenshots/adversarial.png)

---

### Perturbation Pattern
Noise added using gradient direction.

![Perturbation](screenshots/perturbation.png)

---

## ğŸ” Observations

- The adversarial image still visually appears as digit 9.
- The CNN confidently predicts it as digit 4.
- This demonstrates model vulnerability to gradient-based attacks.

---

## ğŸ›¡ Real-World Implication

Such attacks highlight vulnerabilities in:
- Autonomous driving systems
- Facial recognition
- Medical AI diagnosis
- Security systems

Defenses include:
- Adversarial training
- Defensive distillation
- Input preprocessing techniques

---

## ğŸ§© Technologies Used

- Python
- TensorFlow / Keras
- NumPy
- Matplotlib
- MNIST Dataset

---

## ğŸ“Œ Author

Aman Lodha  
Integrated M.Tech CSE  
Focus: Systems, Security, and Applied AI
